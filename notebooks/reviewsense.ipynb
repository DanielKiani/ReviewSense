{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1754f3bb",
   "metadata": {},
   "source": [
    "# 🛍️ ReviewSense: Product Review Analysis Engine\n",
    "\n",
    "> *ReviewSense is a comprehensive, end-to-end Natural Language Processing application built to extract deep, actionable insights from unstructured product reviews.*  \n",
    "Where a simple star rating only tells part of the story, ReviewSense dives into the text to uncover what customers are saying, why they're saying it, and how they feel about specific product features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d383d6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263bc02",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_and_preprocess_reviews(\n",
    "    train_path='data/train.csv', \n",
    "    test_path='data/test.csv',\n",
    "    output_dir='data'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the Amazon Sentiment Analysis dataset (https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews)\n",
    "    (you need to extract the train/test splits from the zip file in the data folder),\n",
    "    performs basic EDA, and preprocesses it for model training.\n",
    "\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the testing CSV file.\n",
    "        output_dir (str): Directory to save the processed file.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    # This dataset typically comes without headers. We'll assign them.\n",
    "    # Column 1: Sentiment (1 = Negative, 2 = Positive)\n",
    "    # Column 2: Title\n",
    "    # Column 3: Review Text\n",
    "    print(f\"Loading data from '{train_path}' and '{test_path}'...\")\n",
    "    try:\n",
    "        col_names = ['sentiment_orig', 'title', 'review']\n",
    "        train_df = pd.read_csv(train_path, header=None, names=col_names)\n",
    "        test_df = pd.read_csv(test_path, header=None, names=col_names)\n",
    "        \n",
    "        # Combine for unified EDA and preprocessing\n",
    "        df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: Make sure '{train_path}' and '{test_path}' are in the specified directory.\")\n",
    "        print(\"This script is designed for the 'Amazon Reviews for Sentiment Analysis' dataset from Kaggle.\")\n",
    "        return\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # --- 2. Preprocessing ---\n",
    "    print(\"\\n--- Preprocessing Data for Sentiment Analysis ---\")\n",
    "\n",
    "    # a) Create new sentiment labels (0 = Negative, 1 = Positive)\n",
    "    # This dataset is binary, not three-class like the previous one.\n",
    "    df['sentiment'] = df['sentiment_orig'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "    # b) Combine title and review body\n",
    "    df['full_text'] = df['title'].astype(str) + \". \" + df['review'].astype(str)\n",
    "\n",
    "    # c) Select and rename columns\n",
    "    processed_df = df[['full_text', 'sentiment']].copy()\n",
    "\n",
    "    # --- 4. Save Processed Data ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, 'reviews_processed.csv')\n",
    "    processed_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved {len(processed_df)} processed reviews to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Preprocess the Reviews Dataset ---\n",
    "print(\"\\n--- Preprocessing started ---\")\n",
    "explore_and_preprocess_reviews()\n",
    "print(\"\\n--- Preprocessing finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c381d73",
   "metadata": {},
   "source": [
    "## Define a base model (Multinomial Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_sentiment_model(data_path='data/reviews_processed.csv', grid_search=True, nb__alpha=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), sample_size: int = 50000):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a Multinomial Naive Bayes model for sentiment analysis.\n",
    "    Can optionally perform a grid search.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the processed reviews CSV file.\n",
    "        grid_search (bool): If True, performs a grid search.\n",
    "        nb__alpha (float): Alpha for MultinomialNB.\n",
    "        tfidf__max_df (float): max_df for TfidfVectorizer.\n",
    "        tfidf__ngram_range (tuple): ngram_range for TfidfVectorizer.\n",
    "        sample_size (int, optional): Number of reviews to use. If None, uses all.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading data from '{data_path}'...\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"\\nERROR: '{data_path}' not found. Please run the EDA script first!\")\n",
    "        return\n",
    "        \n",
    "    df = pd.read_csv(data_path)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # --- 2. Sample Data ---\n",
    "    if sample_size:\n",
    "        print(f\"Using a sample of {sample_size} reviews for training the baseline model.\")\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # --- 3. Train-Test Split ---\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['full_text'],\n",
    "        df['sentiment'],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['sentiment']\n",
    "    )\n",
    "\n",
    "    # --- 4. Create a Pipeline ---\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('nb', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    best_params = None\n",
    "\n",
    "    if grid_search:\n",
    "        # --- 5a. Perform Grid Search ---\n",
    "        print(\"Performing Grid Search to find the best hyperparameters...\")\n",
    "        parameters = {\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "            'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "            'nb__alpha': [0.1, 0.5, 1.0],\n",
    "        }\n",
    "        param_grid = list(ParameterGrid(parameters))\n",
    "        best_score = -1\n",
    "\n",
    "        for params in tqdm(param_grid, desc=\"Grid Search Progress\"):\n",
    "            pipeline.set_params(**params)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            score = pipeline.score(X_test, y_test)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"\\nBest score on test set: {best_score:.4f}\")\n",
    "        print(\"Best parameters found:\")\n",
    "        print(best_params)\n",
    "\n",
    "    else:\n",
    "        # --- 5b. Use provided hyperparameters ---\n",
    "        print(\"Skipping grid search and using provided hyperparameters...\")\n",
    "        best_params = {\n",
    "            'nb__alpha': nb__alpha,\n",
    "            'tfidf__max_df': tfidf__max_df,\n",
    "            'tfidf__ngram_range': tfidf__ngram_range\n",
    "        }\n",
    "\n",
    "    # --- 6. Train the Final Model ---\n",
    "    print(\"\\nTraining final model...\")\n",
    "    best_model = pipeline.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- 7. Evaluate the Best Model ---\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    target_names = ['Negative', 'Positive']\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Confusion Matrix for Naive Bayes on Amazon Reviews')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Train the base model ---\n",
    "train_baseline_sentiment_model(sample_size=150000, grid_search=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5e4ba",
   "metadata": {},
   "source": [
    "## Define the dataset and lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Amazon Reviews.\n",
    "\n",
    "    This class takes a pandas DataFrame of review data, a tokenizer, and a max\n",
    "    token length, and prepares it for use in a PyTorch model. It handles the\n",
    "    tokenization of the text and the formatting of the labels for each item.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer: The Hugging Face tokenizer to use for processing text.\n",
    "        data (pd.DataFrame): The DataFrame containing the review data.\n",
    "        max_token_len (int): The maximum sequence length for the tokenizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_token_len: int):\n",
    "        \"\"\"\n",
    "        Initializes the ReviewDataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing 'full_text' and\n",
    "                                 'sentiment' columns.\n",
    "            tokenizer: The pre-trained tokenizer instance.\n",
    "            max_token_len (int): The maximum length for tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Retrieves one sample from the dataset at the specified index.\n",
    "\n",
    "        This method handles the tokenization of a single review text, including\n",
    "        padding and truncation, and formats the output into a dictionary of\n",
    "        tensors ready for the model.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the data sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the tokenized inputs and the label,\n",
    "                  with the following keys:\n",
    "                  - 'input_ids': The token IDs of the review text.\n",
    "                  - 'attention_mask': The attention mask for the review text.\n",
    "                  - 'labels': The sentiment label as a tensor.\n",
    "        \"\"\"\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = str(data_row.full_text)\n",
    "        labels = data_row.sentiment\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=torch.tensor(labels, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "class ReviewDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning DataModule to handle the Amazon Reviews dataset.\n",
    "\n",
    "    This class encapsulates all the steps needed to process the data:\n",
    "    loading, splitting, and creating PyTorch DataLoaders for training,\n",
    "    validation, and testing. It allows for using a smaller random sample of the\n",
    "    full dataset for faster experimentation.\n",
    "\n",
    "    Attributes:\n",
    "        data_path (str): Path to the processed CSV file.\n",
    "        batch_size (int): The size of each data batch.\n",
    "        max_token_len (int): The maximum sequence length for the tokenizer.\n",
    "        tokenizer: The Hugging Face tokenizer instance.\n",
    "        num_workers (int): The number of CPU cores to use for data loading.\n",
    "        sample_size (int, optional): The number of samples to use. If None,\n",
    "                                     the full dataset is used.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str, batch_size: int = 16, max_token_len: int = 256, model_name='distilbert-base-uncased', num_workers: int = 0, sample_size: int = None):\n",
    "        \"\"\"\n",
    "        Initializes the ReviewDataModule.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): The path to the processed CSV data file.\n",
    "            batch_size (int): The number of samples per batch.\n",
    "            max_token_len (int): Maximum length of tokenized sequences.\n",
    "            model_name (str): The name of the pre-trained model to use for the tokenizer.\n",
    "            num_workers (int): Number of subprocesses to use for data loading.\n",
    "            sample_size (int, optional): If specified, a random sample of this\n",
    "                                         size will be used from the dataset.\n",
    "                                         Defaults to None, which uses the full dataset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.num_workers = num_workers\n",
    "        self.sample_size = sample_size\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Loads and splits the data for training, validation, and testing.\n",
    "\n",
    "        This method is called by PyTorch Lightning. It reads the CSV, handles\n",
    "        missing values, optionally takes a random sample, and performs a\n",
    "        stratified train-validation-test split. The indices of the resulting\n",
    "        DataFrames are reset to prevent potential KeyErrors during data loading.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # If a sample size is provided, sample the dataframe\n",
    "        if self.sample_size:\n",
    "            print(f\"Using a sample of {self.sample_size} reviews.\")\n",
    "            df = df.sample(n=self.sample_size, random_state=42)\n",
    "\n",
    "        # Stratified split to maintain label distribution\n",
    "        train_val_df, self.test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df.sentiment)\n",
    "        self.train_df, self.val_df = train_test_split(train_val_df, test_size=0.1, random_state=42, stratify=train_val_df.sentiment)\n",
    "\n",
    "        # Reset indices to prevent KeyErrors\n",
    "        self.train_df = self.train_df.reset_index(drop=True)\n",
    "        self.val_df = self.val_df.reset_index(drop=True)\n",
    "        self.test_df = self.test_df.reset_index(drop=True)\n",
    "\n",
    "        print(f\"Size of training set: {len(self.train_df)}\")\n",
    "        print(f\"Size of validation set: {len(self.val_df)}\")\n",
    "        print(f\"Size of test set: {len(self.test_df)}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        return DataLoader(\n",
    "            ReviewDataset(self.train_df, self.tokenizer, self.max_token_len),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        return DataLoader(\n",
    "            ReviewDataset(self.val_df, self.tokenizer, self.max_token__len),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the test set.\"\"\"\n",
    "        return DataLoader(\n",
    "            ReviewDataset(self.test_df, self.tokenizer, self.max_token_len),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ac47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "data_path = \"data/reviews_processed.csv\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_TOKEN_LEN = 256\n",
    "\n",
    "print(\"Initializing ReviewDataModule...\")\n",
    "review_datamodule = ReviewDataModule(\n",
    "    data_path=data_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_LEN,\n",
    "    model_name='distilbert-base-uncased',\n",
    "    sample_size=100000 # Pass the sample size to the datamodule\n",
    ")\n",
    "review_datamodule.setup()\n",
    "\n",
    "# Fetch one batch from the training dataloader to inspect its contents\n",
    "print(\"\\n--- Fetching one batch from the training dataloader ---\")\n",
    "train_batch = next(iter(review_datamodule.train_dataloader()))\n",
    "\n",
    "print(\"\\n--- Example Batch ---\")\n",
    "print(f\"Input IDs shape: {train_batch['input_ids'].shape}\")\n",
    "print(f\"Attention Mask shape: {train_batch['attention_mask'].shape}\")\n",
    "print(f\"Labels: {train_batch['labels']}\")\n",
    "print(f\"Labels shape: {train_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7781f4",
   "metadata": {},
   "source": [
    "## FineTune DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for the sentiment classification model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilbert-base-uncased', n_classes=2, learning_rate=2e-5, n_warmup_steps=0, n_training_steps=0, dropout_prob=0.2): # Added dropout\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Configure dropout\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = dropout_prob\n",
    "        config.attention_probs_dropout_prob = dropout_prob\n",
    "        config.num_labels = n_classes\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward(**batch)\n",
    "        self.log(\"train_loss\", output.loss, prog_bar=True, logger=True)\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.forward(**batch)\n",
    "        preds = torch.argmax(output.logits, dim=1)\n",
    "        val_acc = accuracy(preds, batch['labels'], task='binary')\n",
    "        self.log(\"val_loss\", output.loss, prog_bar=True, logger=True)\n",
    "        self.log(\"val_accuracy\", val_acc, prog_bar=True, logger=True)\n",
    "        return output.loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self.forward(**batch)\n",
    "        preds = torch.argmax(output.logits, dim=1)\n",
    "        test_acc = accuracy(preds, batch['labels'], task='binary')\n",
    "        self.log(\"test_accuracy\", test_acc)\n",
    "        return test_acc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        output = self.forward(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        return torch.argmax(output.logits, dim=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=0.01)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.n_warmup_steps,\n",
    "            num_training_steps=self.hparams.n_training_steps\n",
    "        )\n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiment_model(data_path='data/reviews_processed.csv', model_name='distilbert-base-uncased', n_epochs=5, sample_size: int = None):\n",
    "    \"\"\"\n",
    "    Main function to train the sentiment analysis model on the Amazon Reviews dataset.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the processed data file.\n",
    "        model_name (str): Name of the transformer model to use.\n",
    "        n_epochs (int): Maximum number of epochs for training.\n",
    "        sample_size (int, optional): The number of reviews to use for training.\n",
    "                                     If None, the full dataset is used.\n",
    "    \"\"\"\n",
    "    # --- 1. Hyperparameters ---\n",
    "    BATCH_SIZE = 64\n",
    "    MAX_TOKEN_LEN = 256\n",
    "    LEARNING_RATE = 2e-5\n",
    "    N_CLASSES = 2  # Negative, Positive\n",
    "\n",
    "    # --- 2. Initialize DataModule ---\n",
    "    print(\"Initializing ReviewDataModule...\")\n",
    "    review_datamodule = ReviewDataModule(\n",
    "        data_path=data_path,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_token_len=MAX_TOKEN_LEN,\n",
    "        model_name=model_name,\n",
    "        sample_size=sample_size # Pass the sample size to the datamodule\n",
    "    )\n",
    "    review_datamodule.setup()\n",
    "\n",
    "    n_training_steps = len(review_datamodule.train_dataloader()) * n_epochs\n",
    "    n_warmup_steps = int(n_training_steps * 0.1)\n",
    "\n",
    "    # --- 3. Initialize Model ---\n",
    "    print(\"Initializing SentimentClassifier model...\")\n",
    "    model = SentimentClassifier(\n",
    "        model_name=model_name,\n",
    "        n_classes=N_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        n_warmup_steps=n_warmup_steps,\n",
    "        n_training_steps=n_training_steps\n",
    "    )\n",
    "\n",
    "    # --- 4. Configure Training Callbacks ---\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"sentiment-binary-best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"sentiment-classifier-binary\")\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    # --- 5. Initialize Trainer ---\n",
    "    print(\"Initializing PyTorch Lightning Trainer...\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        max_epochs=n_epochs,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "    )\n",
    "\n",
    "    # --- 6. Start Training ---\n",
    "    print(f\"Starting training with {model_name} for up to {n_epochs} epochs...\")\n",
    "    trainer.fit(model, review_datamodule)\n",
    "\n",
    "    # --- 7. Evaluate on Test Set and Generate Confusion Matrix ---\n",
    "    print(\"\\nTraining complete. Evaluating on the test set...\")\n",
    "    trainer.test(model, datamodule=review_datamodule)\n",
    "\n",
    "    predictions = trainer.predict(model, datamodule=review_datamodule)\n",
    "    if predictions:\n",
    "        all_preds = torch.cat(predictions).cpu().numpy()\n",
    "        true_labels = review_datamodule.test_df.sentiment.to_numpy()\n",
    "        target_names = ['Negative', 'Positive'] # Updated labels\n",
    "\n",
    "        cm = confusion_matrix(true_labels, all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n",
    "                    xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.title('Confusion Matrix for Sentiment Analysis')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Train DistilBert ---\n",
    "train_sentiment_model(data_path=data_path, sample_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc7315",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSummarizer:\n",
    "    \"\"\"\n",
    "    A class to handle the summarization of product reviews using a pre-trained T5 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='t5-small'):\n",
    "        \"\"\"\n",
    "        Initializes the summarizer with a pre-trained T5 model and tokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained T5 model to use.\n",
    "        \"\"\"\n",
    "        print(f\"Loading summarization model: {model_name}...\")\n",
    "        self.model_name = model_name\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # Load the tokenizer and model from Hugging Face\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "        print(\"Summarization model loaded successfully.\")\n",
    "\n",
    "    def summarize(self, text: str, max_length: int = 50, min_length: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Generates a summary for a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The review text to summarize.\n",
    "            max_length (int): The maximum length of the generated summary.\n",
    "            min_length (int): The minimum length of the generated summary.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated summary.\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # T5 models require a prefix for the task. For summarization, it's \"summarize: \"\n",
    "        preprocess_text = f\"summarize: {text.strip()}\"\n",
    "\n",
    "        # Tokenize the input text\n",
    "        tokenized_text = self.tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = self.model.generate(\n",
    "            tokenized_text,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode the summary and return it\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "class AspectAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to handle Aspect-Based Sentiment Analysis (ABSA) using a pre-trained model.\n",
    "    \"\"\"\n",
    "    # Changed to a different, currently valid lightweight model for ABSA.\n",
    "    def __init__(self, model_name='yangheng/deberta-v3-base-absa-v1.1', force_cpu=False):\n",
    "        \"\"\"\n",
    "        Initializes the ABSA pipeline with a pre-trained model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained ABSA model.\n",
    "            force_cpu (bool): If True, forces the model to run on the CPU.\n",
    "        \"\"\"\n",
    "        print(f\"Loading Aspect-Based Sentiment Analysis model: {model_name}...\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if force_cpu:\n",
    "            self.device = -1 # Use -1 for CPU in pipeline\n",
    "            print(\"Forcing ABSA model to run on CPU.\")\n",
    "        else:\n",
    "            self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "        print(f\"Using device: {self.device} (0 for GPU, -1 for CPU)\")\n",
    "\n",
    "        self.absa_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=self.model_name,\n",
    "            tokenizer=self.model_name,\n",
    "            device=self.device\n",
    "        )\n",
    "        print(\"ABSA model loaded successfully.\")\n",
    "\n",
    "    def analyze(self, text: str, aspects: list) -> dict:\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment towards a list of aspects within a given text.\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str) or not aspects:\n",
    "            return {}\n",
    "\n",
    "        # The model expects the review and aspect separated by a special token.\n",
    "        # Note: Different ABSA models might expect different input formats.\n",
    "        # This format is common but may need adjustment for other models.\n",
    "        inputs = [f\"{text} [SEP] {aspect}\" for aspect in aspects]\n",
    "        results = self.absa_pipeline(inputs)\n",
    "\n",
    "        # Process results into a user-friendly dictionary\n",
    "        aspect_sentiments = {}\n",
    "        for aspect, result in zip(aspects, results):\n",
    "            aspect_sentiments[aspect] = {'sentiment': result['label'], 'score': result['score']}\n",
    "\n",
    "        return aspect_sentiments\n",
    "\n",
    "class FineTunedSentimentClassifier:\n",
    "    \"\"\"\n",
    "    This class handles loading the fine-tuned checkpoint and making predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path, model_name='distilbert-base-uncased', force_cpu=False):\n",
    "        self.device = 'cpu' if force_cpu else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading fine-tuned sentiment model from checkpoint: {checkpoint_path}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.model = SentimentClassifier.load_from_checkpoint(checkpoint_path, map_location=self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.labels = ['NEGATIVE', 'POSITIVE']\n",
    "        print(\"Fine-tuned sentiment model loaded successfully.\")\n",
    "\n",
    "    def classify(self, text: str) -> dict:\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=128,\n",
    "            return_token_type_ids=False, padding=\"max_length\",\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(self.device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        prediction_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        return {'label': self.labels[prediction_idx], 'score': probabilities[0][prediction_idx].item()}\n",
    "\n",
    "class AspectExtractor:\n",
    "    \"\"\"\n",
    "    This class uses a Part-of-Speech (POS) tagging model to first extract all\n",
    "    potential aspect terms (nouns) from a review text. It then filters these\n",
    "    nouns against a pre-defined dictionary of valid aspects for a given\n",
    "    product category to return only the relevant features.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"vblagoje/bert-english-uncased-finetuned-pos\", force_cpu=False):\n",
    "        self.model_name = model_name\n",
    "        self.device = 'cpu' if force_cpu else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading Part-of-Speech (POS) tagging model: {self.model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.pipeline = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=self.model_name,\n",
    "            device=-1 if self.device == 'cpu' else 0,\n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "        print(\"POS tagging model loaded successfully.\")\n",
    "\n",
    "    def extract(self, text: str, aspect_dictionary: list) -> list:\n",
    "        \"\"\"\n",
    "        Extracts aspects from the given text that are present in the provided\n",
    "        aspect dictionary.\n",
    "\n",
    "        Args:\n",
    "            text (str): The review text to analyze.\n",
    "            aspect_dictionary (list): A list of valid, known aspects for the\n",
    "                                      product category.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of aspects that were both found in the text and are\n",
    "                  present in the aspect dictionary.\n",
    "        \"\"\"\n",
    "        if not text or not aspect_dictionary:\n",
    "            return []\n",
    "\n",
    "        # 1. Extract all nouns from the text using the POS model\n",
    "        model_outputs = self.pipeline(text)\n",
    "        noun_tags = {'NOUN', 'PROPN'}\n",
    "        extracted_nouns = {\n",
    "            output['word'].lower() for output in model_outputs\n",
    "            if output['entity_group'] in noun_tags\n",
    "        }\n",
    "\n",
    "        # 2. Filter the extracted nouns against the provided dictionary\n",
    "        # We find the intersection between the two sets.\n",
    "        valid_aspects = {aspect.lower() for aspect in aspect_dictionary}\n",
    "\n",
    "        final_aspects = list(extracted_nouns.intersection(valid_aspects))\n",
    "\n",
    "        return final_aspects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc21c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# --- IMPORTANT: UPDATE THIS PATH ---\n",
    "# You need to provide the path to the best checkpoint file that was saved\n",
    "# during the training of your sentiment model.\n",
    "SENTIMENT_CHECKPOINT_PATH = \"checkpoints/sentiment-binary-best-checkpoint.ckpt\"\n",
    "\n",
    "# --- Pre-defined Aspect Dictionaries for Different Product Categories ---\n",
    "ASPECT_DICTIONARIES = {\n",
    "    \"Phone\": ['camera', 'battery', 'battery life', 'screen', 'performance', 'price', 'design'],\n",
    "    \"Coffee Maker\": ['ease of use', 'design', 'noise level', 'coffee quality', 'brew time', 'cleaning'],\n",
    "    \"Book\": ['plot', 'characters', 'writing style', 'pacing', 'ending'],\n",
    "    \"Default\": ['quality', 'price', 'service', 'design', 'features'] # A fallback list\n",
    "}\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the command-line review analysis tool.\n",
    "    \"\"\"\n",
    "    # --- 1. Load All Models ---\n",
    "    print(\"--- Initializing all models ---\")\n",
    "    sentiment_classifier, summarizer, aspect_analyzer, aspect_extractor = None, None, None, None\n",
    "    try:\n",
    "        summarizer = ReviewSummarizer(force_cpu=True)\n",
    "        aspect_analyzer = AspectAnalyzer(force_cpu=True)\n",
    "        aspect_extractor = AspectExtractor(force_cpu=True)\n",
    "\n",
    "        if not os.path.exists(SENTIMENT_CHECKPOINT_PATH):\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(\"!!! WARNING: Sentiment checkpoint path not found or not set.         !!!\")\n",
    "            print(f\"!!! Please update the 'SENTIMENT_CHECKPOINT_PATH' variable in main.py\")\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        else:\n",
    "            sentiment_classifier = FineTunedSentimentClassifier(\n",
    "                checkpoint_path=SENTIMENT_CHECKPOINT_PATH, force_cpu=True\n",
    "            )\n",
    "        print(\"\\n--- All models loaded successfully ---\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model initialization: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Interactive Loop ---\n",
    "    while True:\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"          Product Review Analysis Tool          \")\n",
    "        print(\"==================================================\")\n",
    "\n",
    "        # Get user input\n",
    "        review_text = input(\"Enter the product review text (or type 'quit' to exit):\\n> \")\n",
    "        if review_text.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        print(\"\\nAvailable Product Categories:\")\n",
    "        for i, category in enumerate(ASPECT_DICTIONARIES.keys(), 1):\n",
    "            print(f\"{i}. {category}\")\n",
    "\n",
    "        category_choice = input(f\"Select a product category (1-{len(ASPECT_DICTIONARIES)}):\\n> \")\n",
    "        try:\n",
    "            category_idx = int(category_choice) - 1\n",
    "            product_category = list(ASPECT_DICTIONARIES.keys())[category_idx]\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid choice. Using 'Default' category.\")\n",
    "            product_category = \"Default\"\n",
    "\n",
    "        # --- 3. Run Analysis ---\n",
    "        print(\"\\n--- Analyzing Review... ---\")\n",
    "\n",
    "        # a. Overall Sentiment\n",
    "        sentiment_result = sentiment_classifier.classify(review_text)\n",
    "\n",
    "        # b. Summary\n",
    "        summary_result = summarizer.summarize(review_text)\n",
    "\n",
    "        # c. Aspect Extraction and Analysis\n",
    "        aspect_dictionary = ASPECT_DICTIONARIES.get(product_category)\n",
    "        extracted_aspects = aspect_extractor.extract(review_text, aspect_dictionary)\n",
    "        aspect_results = None\n",
    "        if extracted_aspects:\n",
    "            aspect_results = aspect_analyzer.analyze(review_text, extracted_aspects)\n",
    "\n",
    "        # --- 4. Display Results ---\n",
    "        print(\"\\n-------------------- ANALYSIS RESULTS --------------------\")\n",
    "        print(f\"\\n[ Overall Sentiment ]\")\n",
    "        print(f\"  - Sentiment: {sentiment_result['label']} (Score: {sentiment_result['score']:.2f})\")\n",
    "\n",
    "        print(f\"\\n[ Generated Summary ]\")\n",
    "        print(f\"  - {summary_result}\")\n",
    "\n",
    "        print(f\"\\n[ Detected Aspect Sentiments ]\")\n",
    "        if aspect_results:\n",
    "            for aspect, result in aspect_results.items():\n",
    "                print(f\"  - {aspect.title()}: {result['sentiment']} (Score: {result['score']:.2f})\")\n",
    "        else:\n",
    "            print(\"  - No relevant aspects from the dictionary were detected in the review.\")\n",
    "        print(\"----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71257428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the workflow ---\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
